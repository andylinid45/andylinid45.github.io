<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Big-data | Andy Lin]]></title>
  <link href="http://andylinid45.github.io/blog/categories/big-data/atom.xml" rel="self"/>
  <link href="http://andylinid45.github.io/"/>
  <updated>2015-10-07T22:43:32+08:00</updated>
  <id>http://andylinid45.github.io/</id>
  <author>
    <name><![CDATA[Andy]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Ruby on Hadoop, Using Streaming]]></title>
    <link href="http://andylinid45.github.io/blog/2015/08/22/ruby-on-hadoop/"/>
    <updated>2015-08-22T15:23:05+08:00</updated>
    <id>http://andylinid45.github.io/blog/2015/08/22/ruby-on-hadoop</id>
    <content type="html"><![CDATA[<p>wcmapper.rb
```</p>

<h1>!/usr/bin/ruby  -w</h1>

<p>while line = gets</p>

<pre><code>words = line.split(" ")
words.each{ |word| puts word.strip+"\t1"}
</code></pre>

<p>end</p>

<p>```</p>

<p>wcreducer.rb
```</p>

<h1>!/usr/bin/ruby  -w</h1>

<p>current = nil
count = 0</p>

<p>while line = gets</p>

<pre><code>word, counter = line.split("\t")

if word == current
    count = count+1
else
    puts current+"\t"+count.to_s if current
    current = word
    count = 1
end
</code></pre>

<p>end
puts current+&ldquo;\t&rdquo;+count.to_s
```</p>

<p>Implementing WordCount using hadoop streaming
```
hadoop fs -copyFromLocal test.txt /user/andy</p>

<p>hadoop jar /opt/hadoop/contrib/streaming/hadoop-streaming-1.0.4.jar -file wcmapper.rb -mapper wcmapper.rb -file wcreducer.rb -reducer wcreducer.rb -input test.txt -output output</p>

<p>hadoop fs -cat output/part-00000
```</p>

<p>Performing the analysis from the command line
<code>
cat test.txt | ./wcmapper.rb | sort | ./wcreducer.rb
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[大数据在电子商务的应用]]></title>
    <link href="http://andylinid45.github.io/blog/2015/08/22/what-big-data-can-help-e-commerce/"/>
    <updated>2015-08-22T14:18:33+08:00</updated>
    <id>http://andylinid45.github.io/blog/2015/08/22/what-big-data-can-help-e-commerce</id>
    <content type="html"><![CDATA[<p><code>
流量统计
页面PV，用户IP
统计网站、APP客户分布
日、月度注册统计
日、月度购买次数统计
促销事件效果统计
基于浏览记录推荐商品
基于购买记录推荐商品
热销商品预测（可能）
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using HDFS FS Shell]]></title>
    <link href="http://andylinid45.github.io/blog/2015/08/22/using-hdfs-fs-shell/"/>
    <updated>2015-08-22T11:00:09+08:00</updated>
    <id>http://andylinid45.github.io/blog/2015/08/22/using-hdfs-fs-shell</id>
    <content type="html"><![CDATA[<p>```
hadoop fs -mkdir /user
hadoop fs -mkdir /user/andy
hadoop fs -ls /user</p>

<p>echo &ldquo;This is a test.&rdquo; >> test.txt
hadoop dfs -copyFromLocal test.txt /user/andy
hadoop dfs -ls /user/andy
hadoop dfs -cat /user/andy/test.txt</p>

<p>hadoop dfs -mkdir data
hadoop dfs -cp test.txt data
hadoop dfs -ls data
hadoop jar /opt/hadoop/hadoop-examples-1.0.4.jar wordcount data out
hadoop fs -ls out
hadoop fs -cat out/part-r-00000
hadoop jar /opt/hadoop/hadoop-examples-1.0.4.jar
```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Config Hadoop for Pseudo Distributed Mode]]></title>
    <link href="http://andylinid45.github.io/blog/2015/08/21/config-hadoop-for-pseudo-distributed-mode/"/>
    <updated>2015-08-21T17:17:03+08:00</updated>
    <id>http://andylinid45.github.io/blog/2015/08/21/config-hadoop-for-pseudo-distributed-mode</id>
    <content type="html"><![CDATA[<p>install ssh
<code>
sudo apt-get install openssh-client=1:6.6p1-2ubuntu1
sudo apt-get install openssh-server
ssh-keygen
cp .ssh/id_rsa.pub .ssh/authorized_keys
ssh localhost
</code></p>

<p>sudo nano /opt/hadoop/conf/core-site.xml
```
<property>
<name>fs.default.name</name>
<value>hdfs://localhost:9000</value>
</property></p>

<p><property>
<name>hadoop.tmp.dir</name>
<value>/var/lib/hadoop</value>
</property>
```</p>

<p>sudo nano /opt/hadoop/conf/hdfs-site.xml
<code>
&lt;property&gt;
&lt;name&gt;dfs.replication&lt;/name&gt;
&lt;value&gt;1&lt;/value&gt;
&lt;/property&gt;
</code></p>

<p>sudo nano /opt/hadoop/conf/mapred-site.xml
<code>
&lt;property&gt;
&lt;name&gt;mapred.job.tracker&lt;/name&gt;
&lt;value&gt;localhost:9001&lt;/value&gt;
&lt;/property&gt;
</code></p>

<p>make temp dir for hadoop
<code>
sudo mkdir /var/lib/hadoop
sudo chmod 777 /var/lib/hadoop
</code></p>

<p>make log dir for hadoop
<code>
sudo mkdir /opt/hadoop/logs
sudo chmod 777 /opt/hadoop/logs/
</code></p>

<p>run just for the first time to format file system
<code>
hadoop namenode -format
</code></p>

<p>start it
<code>
start-dfs.sh
start-mapred.sh
hadoop fs -ls /  # have a test
</code></p>

<p>web UI for it
<code>
http://localhost:50030/
http://localhost:50070/
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Install hadoop1.0.4 on Ubuntu14]]></title>
    <link href="http://andylinid45.github.io/blog/2015/08/21/install-hadoop1-dot-0-4-on-ubuntu14/"/>
    <updated>2015-08-21T16:33:46+08:00</updated>
    <id>http://andylinid45.github.io/blog/2015/08/21/install-hadoop1-dot-0-4-on-ubuntu14</id>
    <content type="html"><![CDATA[<p>Download hadoop
<code>
sudo cp hadoop-1.0.4-bin.tar.gz /opt
cd /opt
sudo tar -vxf hadoop-1.0.4-bin.tar.gz
sudo rm hadoop-1.0.4-bin.tar.gz
sudo mv hadoop-1.0.4/ hadoop
</code></p>

<p>sudo gedit /opt/hadoop/conf/hadoop-env.sh  #modify java home
<code>
export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_51
</code></p>

<p>gedit ~/.bashrc # do not use &lsquo;HADOOP_HOME&rsquo;
<code>
export HADOOP_PREFIX=/opt/hadoop
export PATH=$HADOOP_PREFIX/bin:$PATH
</code></p>

<p>have a test
<code>
hadoop jar /opt/hadoop/hadoop-examples-1.0.4.jar pi 4 1000
</code></p>
]]></content>
  </entry>
  
</feed>
